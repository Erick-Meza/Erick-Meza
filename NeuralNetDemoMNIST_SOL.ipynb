{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Erick-Meza/Erick-Meza/blob/main/NeuralNetDemoMNIST_SOL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Day 3: Training a model with PyTorch - Handwriting Detection\n",
        "\n",
        "Welcome to day 3 of IOAI Canada training!\n",
        "\n",
        "Today, we will be training our first neural network model using PyTorch. Using the MNIST database, we will create and train a model that will detect handwritten digits from 0-9.\n",
        "\n",
        "The parts of the code you will write will be marked with \"TO-DO\". Ensure you pay attention to the lecture on how to write the code.\n",
        "\n",
        "**Please make a copy of this notebook so your changes are saved.**\n",
        "\n",
        "If you have any questions, do not hesitate to ask questions in the zoom call."
      ],
      "metadata": {
        "id": "YzRPzBJ_PygS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0YZPcxQNGeAL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch import nn, optim"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Downloading and preparing the data\n",
        "\n",
        "Objectives:\n",
        "- Setup two datasets - one for training and one for validation\n",
        "- Creating dataloaders for both datasets to streamline training and validation"
      ],
      "metadata": {
        "id": "5BaoFXlzeaAr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TO-DO: Download the MNIST dataset and create dataloaders for training and testing.\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "# Download the MNIST datasets, and assign them to a training set and testing set\n",
        "train_dataset = datasets.MNIST(\n",
        "    root='train', # The path where the files are stored\n",
        "    train=True, # Are we training or testing? Seperate data is used for either case.\n",
        "    transform=transforms.ToTensor(), # Converts the data to a tensor that is readable by PyTorch\n",
        "    download=True # Indicates we need to download the data\n",
        ")\n",
        "\n",
        "test_dataset = datasets.MNIST(\n",
        "    root='test',\n",
        "    # TO-DO: fill in the rest of the parameters here\n",
        "    train=False, # Are we training or testing? Seperate data is used for either case.\n",
        "    transform=transforms.ToTensor(), # Converts the data to a tensor that is readable by PyTorch\n",
        "    download=True # Indicates we need to download the data\n",
        ")\n",
        "\n",
        "# Create dataloaders for training and testing, so we can easily load data when training and testing\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    dataset=train_dataset, # The dataset used for this loader\n",
        "    batch_size=batch_size, # The batch size used (more details later)\n",
        "    shuffle=True # Shuffles the data\n",
        ")\n",
        "\n",
        "# TO-DO: create a new loader called test_loader that loads the data from test_dataset\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    dataset=test_dataset, # The dataset used for this loader\n",
        "    batch_size=batch_size, # The batch size used (more details later)\n",
        "    shuffle=True # Shuffles the data\n",
        ")"
      ],
      "metadata": {
        "id": "7T6QSjHjHFXI",
        "outputId": "8f0fb9f4-8f1c-4030-c237-9fe6f2ac41c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 52.7MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 1.70MB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 14.7MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 7.40MB/s]\n",
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 53.5MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 1.77MB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 14.9MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 6.00MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Display an image\n",
        "\n",
        "To ensure that our dataset works, we will use the matplotlib library to display one of the images in the dataset."
      ],
      "metadata": {
        "id": "90JpN74ze2QI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TO-DO: Display one image from the training dataset using matplotlib.\n",
        "plt.imshow(train_dataset.data[0], cmap='gray') # cmap = color map\n",
        "plt.title(train_dataset.targets[0])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "a2BbzkZDH7RM",
        "outputId": "d70f8154-8d41-4549-80cb-8567631c5685",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Defining the model, loss function, and optimizer\n",
        "\n",
        "Here, we need to specify the device that will be used (GPU or CPU)\n",
        "\n",
        "We will create a very simple neural network, passing each pixel in the image (28x28) as an input. The network has an output of 10, correspoinding to the 10 digits.\n",
        "\n",
        "The loss function dictates how the loss, or how correct/incorrect the model is, is calculated.\n",
        "\n",
        "The optmizer uses the calculated loss, and modifies the weights of the neural network to reduce this loss."
      ],
      "metadata": {
        "id": "D5WgV812fzqg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TO-DO: Initialize the architecture of the device, model, loss function and optimizer\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = nn.Sequential(nn.Linear(28*28, 28), nn.Linear(28,10)).to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr = 0.005)"
      ],
      "metadata": {
        "id": "PhztTPhFIGkM",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Training the model\n",
        "\n",
        "For each batch of images:\n",
        "- Flatten each image, so that it becomes an array of pixels that can be fed into the network\n",
        "- Get the current model's predictions for each image\n",
        "- Determine how well the model did by calculating its loss. The smaller the loss, the better the model.\n",
        "- Backpropagate the loss and modify the weights of the network.\n",
        "\n",
        "These steps are repeated for every batch of images (in our case, every 100 images, the loss is calculated and weights are changed).\n",
        "\n",
        "Every time the program iterates through the entire dataset, it goes through *one* epoch. Here, we will iterate through the dataset 10 times."
      ],
      "metadata": {
        "id": "8twDH-tdgjUB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model\n",
        "# TO-DO: Fill in the blanks\n",
        "\n",
        "# Initial settings - number of epochs\n",
        "num_epochs = 10\n",
        "\n",
        "# Loop through the entire dataset num_epochs times\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "  model.train() # Set the model to train mode\n",
        "\n",
        "  for i, data in enumerate(train_loader): # Loop through for every batch in _______\n",
        "\n",
        "    # Seperate the data into images and labels\n",
        "    images, labels = data\n",
        "\n",
        "    # Send the image/label tensors to the GPU\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    # TO-DO: The following 6 lines are out of order. Re-arrange them so that they are in order.\n",
        "    # Correct order of operations\n",
        "    images = images.view(images.size(0), -1) # Reshape images before forward pass\n",
        "\n",
        "    pred = model(images) # Perform the forward pass\n",
        "    loss = loss_fn(pred, labels) # Calculate the loss\n",
        "    loss.backward() # Calculate the gradients\n",
        "    optimizer.step() # Update the model's parameters\n",
        "\n",
        "    optimizer.zero_grad() # Zero out gradients before calculating new ones\n",
        "\n",
        "    # TO-DO: Add to this code so that for every 100 batches, the current epoch, batch and loss is printed.\n",
        "    if (i+1) % 100 == 0:\n",
        "      print(f\"Epoch: {epoch+1}/{num_epochs}, Batch: {i+1}, Loss: {loss.item():}\")\n"
      ],
      "metadata": {
        "id": "RtdyKvzYJoEf",
        "outputId": "8f73b631-a67c-4100-cd57-996fc68399c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/10, Batch: 100, Loss: 2.1205341815948486\n",
            "Epoch: 1/10, Batch: 200, Loss: 1.9627799987792969\n",
            "Epoch: 1/10, Batch: 300, Loss: 1.7082089185714722\n",
            "Epoch: 1/10, Batch: 400, Loss: 1.5271964073181152\n",
            "Epoch: 1/10, Batch: 500, Loss: 1.4481779336929321\n",
            "Epoch: 1/10, Batch: 600, Loss: 1.3531490564346313\n",
            "Epoch: 2/10, Batch: 100, Loss: 1.0768141746520996\n",
            "Epoch: 2/10, Batch: 200, Loss: 1.1000183820724487\n",
            "Epoch: 2/10, Batch: 300, Loss: 1.0168532133102417\n",
            "Epoch: 2/10, Batch: 400, Loss: 0.8251692056655884\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-36628dd25d91>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Reshape images before forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Perform the forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Calculate the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Calculate the gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;31m# fmt: off\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1743\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1744\u001b[0m         \u001b[0mforward_call\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m         \u001b[0;31m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Evaluate the model\n",
        "\n",
        "Now that our model is trained, let's see how well our model performs against a dataset it hasn't seen before - the testing dataset.\n",
        "\n",
        "Here, we want to compare the model's prediction for the image with the actual image label (so if an image contains the number 9, we want the model to predict a 9)\n",
        "\n",
        "We calculate the accuracy of each batch using the test_loader, and then evaluate the total accuracy by calculating the average accuracy across all batches."
      ],
      "metadata": {
        "id": "kV9ZsAP1lbtG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "\n",
        "model.eval() # Set the model to evaluation mode\n",
        "\n",
        "with torch.no_grad(): # Let PyTorch know to NOT modify the gradients (used to change weights, we don't want to do that here)\n",
        "\n",
        "    total_accuracy = 0 # Total accuracy used to calculate average batch accuracy at the end\n",
        "    for i, data in enumerate(test_loader): # Loop through for every batch in test_loader\n",
        "\n",
        "        # TO-DO: Output the model prediction for this batch into a variable called test_output. Refer to step 4 on how to do this.\n",
        "         # Send the image/label tensors to the GPU\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        images = images.view(images.size(0), -1)\n",
        "        # TO-DO: Output the model prediction for this batch into a variable called test_output.\n",
        "        test_output = model(images)\n",
        "\n",
        "        pred_y = torch.max(test_output, 1)[1].data.squeeze() # Get the class which has the highest probability (what number the model thinks the image corresponds to)\n",
        "        batch_accuracy = (pred_y == labels).sum().item() / float(labels.size(0)) # Determines the accuracy of the model for this batch\n",
        "        total_accuracy += batch_accuracy # Adds to the concurent accuracy\n",
        "        i += 1\n",
        "\n",
        "    # TO-DO: Complete this line so that it calculates the average accuracy across all batches\n",
        "    accuracy = total_accuracy / len(test_loader)\n",
        "\n",
        "print(f\"Test Accuracy of the model on the 10000 test images: {accuracy:.3f}\")\n"
      ],
      "metadata": {
        "id": "zb9mhLIsNyty",
        "outputId": "ccd2c1f9-a660-4646-e08d-e8de92208f4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy of the model on the 10000 test images: 0.770\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Our own handwriting data\n",
        "\n",
        "Now, we will import our own handwriting, and see if the model can recognize it.\n",
        "\n",
        "To do this, you need to create a 28x28 image with a black background using Microsoft Paint, or any other software.\n",
        "\n",
        "Then, you will need to mount your Google Drive so that Google Colab can access files on your drive. Pay attention to the lecture on how to do this.\n",
        "\n",
        "Afterwards, open the image, preprocess it (convert to grayscale and make it to a tensor) so it can be read by our model, and output its prediction."
      ],
      "metadata": {
        "id": "Lcmtr0urmYVh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import custom image\n",
        "import os\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Open the image (TO-DO: Put the link to the image here)\n",
        "image_path = \"/content/drive/MyDrive/9.png\"\n",
        "image = Image.open(image_path)\n",
        "\n",
        "# Set model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Define a transform to convert the image to a tensor (TO-DO: Fill in the blanks)\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Grayscale(),  # Convert the image to grayscale\n",
        "    transforms.ToTensor(),   # Convert the image to a tensor\n",
        "])\n",
        "\n",
        "# Apply the transform to the image\n",
        "image_tensor = preprocess(image)\n",
        "image_tensor = image_tensor.view(image_tensor.size(0), -1) # Reshape to (batch_size, input_features)\n",
        "with torch.no_grad():\n",
        "# TO-DO: From this new image_tensor, complete the code so that the model predicts the number of thiss image_tensor.\n",
        "  image_tensor = image_tensor.to(device)  # Send tensor to device (GPU or CPU)\n",
        "  output = model(image_tensor)\n",
        "  pred = torch.max(output, 1)[1].item()\n",
        "print(pred);\n",
        "plt.imshow(image, cmap='gray') # cmap = color map\n",
        "plt.title(pred)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "ZL-ulLFTeHCS",
        "outputId": "a0e69c9c-8016-4a25-ccdf-3e7dad905a68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/9.png'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-ae4259a575cc>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Open the image (TO-DO: Put the link to the image here)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mimage_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/9.png\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Set model to evaluation mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3464\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3465\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3466\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3467\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/9.png'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "B_zMVTu9Gj8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: Save & Load our model\n",
        "\n",
        "We don't want to train our model every time we open up our program. Saving our model allows us to retrieve this model at a later time and use it.\n",
        "\n",
        "IMPORTANT: When loading the model, the architecture of the model must be defined first before loading."
      ],
      "metadata": {
        "id": "Ui50ucB8nFu8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TO-DO: Save the model to Google Drive\n",
        "torch.save(model.state_dict(), \"/content/ioaiModel.pth\")"
      ],
      "metadata": {
        "id": "72VSf4EZU7z8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = nn.Sequential(nn.Linear(28*28,28), nn.Linear(28, 10)).to(device)\n",
        "model.load_state_dict(torch.load(\"/content/ioaiModel.pth\"))"
      ],
      "metadata": {
        "id": "paPLx0wxU9q-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P7MusF-oE3dG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}